# Snake Game 实验配置

本目录包含贪吃蛇游戏的实验配置和脚本。

## 环境说明

- **游戏**: 经典贪吃蛇游戏
- **网格大小**: 10x10
- **状态空间**: 100维离散空间 (扁平化的网格)
- **动作空间**: 4个离散动作 (上、下、左、右)
- **目标**: 吃到食物，尽可能长时间活动
- **成功标准**: 平均奖励 >= 50，蛇长度 >= 8

## 游戏规则

- 蛇在网格上移动，目标是吃到食物(F)
- 蛇头(H)和身体(B)
- 碰到墙壁或自己会游戏结束
- 每吃一个食物，蛇的长度增加1

## 奖励设计

```python
+10  : 吃到食物
-1   : 每一步 (鼓励快速吃到食物)
-10  : 碰到墙壁或自己
```

## 算法

使用DQN (Deep Q-Network)算法

## 超参数

```python
gamma = 0.99                # 折扣因子
learning_rate = 0.001       # 学习率
batch_size = 64             # 批次大小
memory_size = 10000         # 经验回放缓冲区大小
target_update_freq = 10     # 目标网络更新频率
hidden_sizes = [256, 256]   # 隐藏层大小 (较大网络)
epsilon_start = 1.0         # 初始探索率
epsilon_end = 0.01          # 最终探索率
epsilon_decay = 0.99        # 探索率衰减
```

## 训练

```bash
python train.py --game snake --episodes 1000
```

## 测试

```bash
python test.py --game snake --model models/snake_best.pth --episodes 10 --render
```

## 预期结果

- 训练轮数: ~1000轮
- 平均奖励: 50-100+
- 蛇平均长度: 6-10
- 训练时间: ~10-15分钟 (CPU)

## 难度分析

贪吃蛇相对更难因为：
1. 状态空间较大 (100维)
2. 需要长期规划避免碰撞
3. 奖励相对稀疏 (只有吃食物才有大奖励)
4. 动作之间有依赖关系 (不能直接反向)

## 改进建议

1. 使用卷积神经网络处理空间信息
2. 实现帧堆叠以获得移动信息
3. 使用更复杂的奖励函数
4. 添加食物位置的相对信息到状态中
5. 考虑使用Double DQN或Dueling DQN
